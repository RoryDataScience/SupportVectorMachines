{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Aim** <br>\n",
    "This notebooks is to apply Support Vector Machines to a Classification problem\n",
    "\n",
    "We will develop a model using Support Vector Machine which should correctly classify the handwritten digits from 0-9 based on the pixel values given as features. The problem as been reformatted as a binary classification problem.\n",
    "\n",
    "**Data Description** \n",
    "For this problem, we use the MNIST data which is a large database of handwritten digits. The 'pixel values' of each digit (image) comprise the features, and the actual number between 0-9 is the label. Each feature represents the pixal intensity from 0 (white) to 255 (black)\n",
    "\n",
    "Since each image is of 28 x 28 pixels, and each pixel forms a feature, there are 784 features. MNIST digit recognition is a well-studied problem in the ML community, and people have trained numerous models (Neural Networks, SVMs, boosted trees etc.) achieving error rates as low as 0.23% (i.e. accuracy = 99.77%, with a convolutional neural network).\n",
    "\n",
    "We'll first explore the dataset a bit, prepare it (scale etc.) and then experiment with linear and non-linear SVMs with various hyperparameters. Requirement to shuffle the data for different CV folds to ensure that each fold contains features for each class \n",
    "\n",
    "**Project Assumptions**\n",
    "\n",
    "**Project Objective**\n",
    "~~- Basic EDA of the MNIST Dataset~~\n",
    "    - ~~How should pixal data be analysed? ~~\n",
    "    - ~~Plot example of the pixel data~~ \n",
    "    - ~~What columns are identical or have very similar distributions?~~\n",
    "- Benchmark model\n",
    "    ~~- Majority Class~~~\n",
    "    ~~- Stochastic Gradient Descent Classifier (good for large datasets and online learning)~~\n",
    "- Understand the framework of support vector machines \n",
    "    ~~- Build a model that overfits a small proporion of the dataset and how to reach this conclusion~~\n",
    "    ~~- Stratified Sampling~~\n",
    "    - Understand methods to select features for classification problems\n",
    "    - Include bias/variance trade off analysis\n",
    "    ~~- Cross Validation Strategy (Shuffle data & Stratified Sample)~~\n",
    "    ~~- Sci-Kit Learns Transformation Pipeline (page 66 - Hands on ML with Sci-kit Learn sklearn.pipeline import Pipeline)~~\n",
    "~~- Application of multiple Kernal Types and model performance analysis~~\n",
    "~~- Understand and apply an appropriate loss function ~~\n",
    "    ~~- Hinge Loss~~\n",
    "~~- Feature Selection/Scaling~~\n",
    "   ~~- Add randomised features~~\n",
    "  ~~- Implement Scaling/Normalisation using Sci-kit learn Pipeline~~\n",
    "- ~~Hyperparameter Tuning (Implement and understand grid search plus evaluate where the model over/underfits)~~\n",
    "    ~~- Grid Search for parameters (page 72 - Hands on ML): Determine which parameters need to be optimised ~~\n",
    "- Investigate if SVM model can be plotted \n",
    "    - Error evaluation: Where does the system commonly make mistakes and how can this be improved?\n",
    "    - Decision boundary\n",
    "~~Decision function in sklearn~~\n",
    "~~- Confusion Matrix ~~\n",
    "~~- Precision-Recall Curve~~\n",
    "~~- RoC Curve~~\n",
    "- Tidy up the final solution \n",
    "    - Neatly store: Date/Data set/Pickle Model/Performance score\n",
    "    - Dockerise\n",
    "- Document brief conclusions \n",
    "    - Think where bias might be incorporated in the model and how this can be treated --- Poor quality measures?\n",
    "    - What is the scope that the model can predict and where could it have issues generalising e.g. new categories?\n",
    "- Document and understand 3 key learning from other kaggle solutions\n",
    "    ~~- https://www.kaggle.com/nishan192/mnist-digit-recognition-using-svm~~\n",
    "    ~~- https://www.kaggle.com/azzion/svm-for-beginners-tutorial~~\n",
    "    ~~- https://www.kaggle.com/fengdanye/machine-learning-4-support-vector-machine~~\n",
    "    ~~- https://www.kaggle.com/residentmario/kernels-and-support-vector-machine-regularization~~\n",
    "\n",
    "**Support Vector Machine Notes**\n",
    "~~- Hands-On Machine Learning with Scikit-Learn & Tensorflow~~\n",
    "~~- Mastering Predictive Analytics with R~~\n",
    "~~- Andrew Ng SVM <br>~~\n",
    "~~https://www.youtube.com/watch?v=XfyR_49hfi8&list=PLLssT5z_DsK-h9vYZkQkYNWcItqhlRJLN&index=74~~\n",
    "\n",
    "**Parameters**\n",
    "~~- Choice of parameter C~~\n",
    "~~- Choice of kernel (similarity function)~~\n",
    "    ~~- Linear SVM uses no Kernel: Standard linear classifier (use when large number of features but small dataset)~~\n",
    "    ~~- Gaussian: Will require selection of parameter sigma^2 (use when small number of features but large training set)~~\n",
    "    ~~- **Perform feature scaling before implementing a gaussian kernel**~~\n",
    "\n",
    "**Algorithm Performance on MNIST Dataset** <br>\n",
    "- For MNIST Data set there is no need to create dummy variables or handle missing data as data set doesn't have any missing data\n",
    "- SVM will be used in relation to a binary classification problem for the initial project \n",
    "- Note that pictures can be sensitive to rotation and imageing shifting which can increase likelihood of error\n",
    "\n",
    "**Model Performances** <br>\n",
    "- Benchmark Model:\n",
    "- Optimal Performance (Personal): \n",
    "- Optimal Performance (Kaggle): Error rates as low as 0.23% (99.77 Accuracy)\n",
    "\n",
    "**Out of Scope**\n",
    "- Missing data treatment \n",
    "- Outlier treatment\n",
    "- Feature transformations and interactions (product/sum etc.)\n",
    "\n",
    "\n",
    "**References** <br>\n",
    "~~https://www.kaggle.com/jwlee508/data-analysis-for-kaggle-mnist~~ <br> \n",
    "~~https://towardsdatascience.com/an-intro-to-kernels-9ff6c6a6a8dc~~ <br>\n",
    "~~https://www.researchgate.net/publication/230800948_The_Secrets_to_Managing_Business_Analytics_Projects~~ <br>\n",
    "https://machinelearningmastery.com/feature-selection-subspace-ensemble-in-python/ <br>\n",
    "https://towardsdatascience.com/one-potential-cause-of-overfitting-that-i-never-noticed-before-a57904c8c89d <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('/Users/Rej1992/Desktop/SVM_Data/mnist_train.csv')\n",
    "test_data = pd.read_csv('/Users/Rej1992/Desktop/SVM_Data/mnist_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order = list(np.sort(train_data['label'].unique()))\n",
    "print(order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_t = train_data.drop([\"label\"], axis=1)\n",
    "Y_t = train_data.label.values\n",
    "\n",
    "plt.figure(figsize=(6,3))\n",
    "X = X_t.to_numpy().reshape(60000, 28, 28)\n",
    "for i in range(8):\n",
    "    plt.subplot(2,4,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(X[i], cmap=plt.cm.binary)\n",
    "    plt.xlabel(str(Y_t[i]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(train_data['label'], kde = False, bins = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[(train_data.label == 1) | (train_data.label == 3)]\n",
    "test_data = test_data[(test_data.label == 1) | (test_data.label == 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a representative sample of the data set\n",
    "train_data = train_data.sample(n = 2000, replace = False) \n",
    "test_data = test_data.sample(n = 2000, replace = False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns \n",
    "train_data = train_data.loc[:,~train_data.columns.duplicated()]\n",
    "test_data = test_data.loc[:,~test_data.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_columns_unique_values(data):\n",
    "    \n",
    "    nunique = data.apply(pd.Series.nunique)\n",
    "    cols_to_drop = nunique[nunique == 1].index\n",
    "    \n",
    "    return data.drop(cols_to_drop, axis=1)\n",
    "\n",
    "test_data = _remove_columns_unique_values(test_data)\n",
    "train_data = _remove_columns_unique_values(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check column overlap\n",
    "print(test_data.columns.difference(train_data.columns))\n",
    "print(train_data.columns.difference(test_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retain only columns that exist in the test data set\n",
    "train_data = train_data.drop(train_data.columns.difference(test_data.columns), axis=1)\n",
    "test_data = test_data.drop(test_data.columns.difference(train_data.columns), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verfiy the data type \n",
    "train_data.dtypes.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing targets\n",
    "train_data = train_data[train_data.label.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse the distribution of the target\n",
    "import seaborn as sns\n",
    "sns.catplot(x=\"label\", kind=\"count\", palette=\"ch:.25\", data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.std().sort_values(ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data.iloc[:, : 50] # Train on first 50 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrmat = train_data.corr()\n",
    "f,ax = plt.subplots(figsize=(12,10))\n",
    "sns.heatmap(corrmat,\n",
    "            vmax=0.8,\n",
    "            square=True,\n",
    "            annot=True,\n",
    "            annot_kws={'size':8})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering \n",
    "## Discretise continuous variables \n",
    "## Create a set of randomised features \n",
    "import random\n",
    "\n",
    "random.seed(1234)\n",
    "train_data['uniform_random'] = np.random.uniform(-1,0,len(train_data))\n",
    "train_data['log_norm_random'] = np.random.lognormal(0, 1, len(train_data))\n",
    "train_data['norm_random'] = np.random.normal(0, 1, len(train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spot check machine learning algorithms on the adult imbalanced dataset\n",
    "# https://machinelearningmastery.com/imbalanced-classification-with-the-adult-income-dataset/\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish X and y & label encode the target variable to have the classes 0 and 1\n",
    "y = LabelEncoder().fit_transform(train_data.label)\n",
    "X = train_data.drop('label', 1)\n",
    "num_ix = X.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation/Standardisation --- Implement Feature Scaling\n",
    "X_scaled = scale(X)\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, train_size = 0.2, random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a benchmark dummy model\n",
    "for strat in ['stratified', 'most_frequent', 'prior', 'uniform']:\n",
    "    dummy_maj = DummyClassifier(strategy=strat).fit(X_train, y_train)\n",
    "    print(cross_val_score(dummy_maj, X_train, y_train, cv = 10, scoring = 'accuracy'))\n",
    "    print(\"Train Stratergy :{} \\n Score :{:.2f}\".format(strat, dummy_maj.score(X_train, y_train)))\n",
    "    print(\"Test Stratergy :{} \\n Score :{:.2f}\".format(strat, dummy_maj.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test: Support Vector Machine \n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    \n",
    "    # DummyClassifier\n",
    "    models.append(DummyClassifier(strategy='uniform'))\n",
    "    names.append('Dummy_Uniform')\n",
    "    \n",
    "    # Stochastic Gradient Descent Classifier\n",
    "    models.append(SGDClassifier(loss=\"hinge\", penalty=\"l2\", max_iter=5))\n",
    "    names.append('Stochastic_GD')\n",
    "    \n",
    "    # SVM Linear Kernel\n",
    "    models.append(SVC(kernel='linear'))\n",
    "    names.append('SVM_Linear')\n",
    "    \n",
    "    # SVM\n",
    "    models.append(SVC(gamma='scale'))\n",
    "    names.append('SVM_Gamma')\n",
    "    \n",
    "    # GBM\n",
    "    models.append(GradientBoostingClassifier(n_estimators=100))\n",
    "    names.append('GBM')\n",
    "    \n",
    "    return models, names\n",
    "\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g--\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0,1])\n",
    "    \n",
    "\n",
    "def plot_roc_curve(fpr, tpr, roc_thresholds, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0,1], [0,1], 'k--')\n",
    "    plt.axis([0,1,0,1])\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "\n",
    "models, names = get_models()\n",
    "results = list()\n",
    "    \n",
    "precisions = []\n",
    "recalls = []\n",
    "thresholds = []\n",
    "\n",
    "# evaluate each model\n",
    "for i in range(len(models)):    \n",
    "    \n",
    "# Fit the model -- Function 1\n",
    "    model = models[i].fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "# Create y_preds -- Function 3\n",
    "    y_preds = model.predict(X_test)\n",
    "    \n",
    "# evaluate the model and store results  -- Function 2\n",
    "    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "    scores = cross_val_score(model, X_train, y_train, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    results.append(scores)\n",
    "        \n",
    "    print(\"\"\"\"\"\")\n",
    "    print('> %s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n",
    "    print(scores)\n",
    "    print(\"Test Data Accuracy:\", metrics.accuracy_score(y_true=y_test, y_pred=y_preds), \"\\n\")\n",
    "    \n",
    "\n",
    "# Confusion Matrix -- Function 4\n",
    "    confusion_matrix(y_test, y_preds)\n",
    "    \n",
    "# Precision/Recall Curves  -- Function 5\n",
    "    if(i != 0):\n",
    "        y_decision_func = cross_val_predict(model,  X_train, y_train, method='decision_function', cv=cv)\n",
    "        precisions, recalls, thresholds = precision_recall_curve(y_train, y_decision_func)\n",
    "        \n",
    "    plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "    plt.show()\n",
    "        \n",
    "# RoC Curve --- Function 6 --- Binary Classifiers & Recall Versus False Positive Rate (Ratio of negative instances that are incorrectly classified as positive)\n",
    "    if(i != 0):\n",
    "        fpr, tpr, roc_thresholds = roc_curve(y_train, y_decision_func)\n",
    "    plot_roc_curve(fpr, tpr, roc_thresholds)\n",
    "    plt.show\n",
    "    \n",
    "# AuC Score --- Function 7\n",
    "    print(roc_auc_score(y_train, y_decision_func))\n",
    "    \n",
    "    \n",
    "\n",
    "# cm\n",
    "    print(metrics.confusion_matrix(y_true=y_test, y_pred=y_preds))\n",
    "    \n",
    "\n",
    "# plot the results\n",
    "plt.boxplot(results, labels=names, showmeans=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_precision_vs_recall(precisions, recalls):\n",
    "#     pass\n",
    "\n",
    "# plot_roc_curve(fpr, tpr)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Attempt differing values of C\n",
    "for this_C in [1,3,5]:\n",
    "    clf = SVC(kernel='linear',C=this_C).fit(X, y) ## Linear only has C parameter\n",
    "    clf2 = LinearSVC(C=this_C).fit(X,y)\n",
    "    scoretrain = clf.score(X,y)\n",
    "    scoretest  = clf.score(X_test, Y_test)\n",
    "    print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,\n",
    "                                                                                          scoretrain,\n",
    "                                                                                          scoretest))\n",
    "\n",
    "# for this_C in [1,3,5,10,40,60,80,100]:\n",
    "#     clf2 = LinearSVC(C=this_C).fit(X,y)\n",
    "#     scoretrain = clf2.score(X,y)\n",
    "#     scoretest  = clf2.score(X_test, Y_test)\n",
    "#     print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,\n",
    "#                                                                                           scoretrain,\n",
    "#                                                                                           scoretest))\n",
    "\n",
    "    \n",
    "# for this_C in [1,3,5,10,40,60,80,100]:\n",
    "#     clf3 = SVC(kernel='rbf',C=this_C).fit(X,y) ## # SMV with RBF KERNAL AND ONLY C PARAMETER \n",
    "#     scoretrain = clf3.score(X,y)\n",
    "#     scoretest  = clf3.score(X_test, Y_test)\n",
    "#     print(\"Linear SVM value of C:{}, training score :{:2f} , Test Score: {:2f} \\n\".format(this_C,\n",
    "#                                                                                           scoretrain,\n",
    "#                                                                                           scoretest))\n",
    "\n",
    "    \n",
    "# # SVM WITH RBF KERNAL, C AND GAMMA HYPERPARAMTER \n",
    "# for this_gamma in [.1,.5,.10,.25,.50,1]:\n",
    "#     for this_C in [1,5,7,10,15,25,50]:\n",
    "#         clf3 = SVC(kernel='rbf',C=this_C,gamma=this_gamma).fit(X_train,Y_train)\n",
    "#         clf3train = clf3.score(X_train,Y_train)\n",
    "#         clf3test  = clf3.score(X_test,Y_test)\n",
    "#         print(\"SVM for Non Linear \\n Gamma: {} C:{} Training Score : {:2f} Test Score : {:2f}\\n\".format(this_gamma,this_C,clf3train,clf3test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid Search\n",
    "param_grid = {'C': [1,5,7,10,15,25,50],\n",
    "              'gamma': [.1,.5,.10,.25,.50,1]}\n",
    "\n",
    "GS = GridSearchCV(SVC(kernel='rbf'),param_grid,cv=5)\n",
    "\n",
    "GS.fit(X_train,Y_train)\n",
    "\n",
    "print(\"the parameters {} are the best.\".format(GS.best_params_))\n",
    "print(\"the best score is {:.2f}.\".format(GS.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise the model decision boundary \n",
    "\n",
    "# custome color maps\n",
    "cm_dark = ListedColormap(['#ff6060', '#8282ff','#ffaa00','#fff244','#4df9b9','#76e8fc','#3ad628'])\n",
    "cm_bright = ListedColormap(['#ffafaf', '#c6c6ff','#ffaa00','#ffe2a8','#bfffe7','#c9f7ff','#9eff93'])\n",
    "\n",
    "plt.scatter(X[:,0],X[:,1],c=y,cmap=cm_dark,s=10,label=y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Visualise the model decision boundary \n",
    "h = .02  # step size in the mesh\n",
    "C_param = 1 # No of neighbours\n",
    "for weights in ['uniform', 'distance']:\n",
    "    # we create an instance of Neighbours Classifier and fit the data.\n",
    "    clf1 = SVC(kernel='linear',C=C_param)\n",
    "    clf1.fit(X, y)\n",
    "\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    x_min, x_max = X[:, 0].min()-.20, X[:, 0].max()+.20\n",
    "    y_min, y_max = X[:, 1].min()-.20, X[:, 1].max()+.20\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    Z = clf1.predict(np.c_[xx.ravel(), yy.ravel()])   # ravel to flatten the into 1D and c_ to concatenate \n",
    "\n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure()\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cm_bright)\n",
    "\n",
    "    # Plot also the training points\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap=cm_dark,\n",
    "                edgecolor='k', s=20)\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.title(\"SVM Linear Classification (kernal = linear, Gamma = '%s')\"% (C_param))\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
